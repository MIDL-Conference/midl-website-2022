---
title: "Masked Autoencoders Pre-training in Multiple Instance Learning for Whole Slide Image Classification"
page_class: "paper"
---

{% import "_macros.html" as macros %}

# Masked Autoencoders Pre-training in Multiple Instance Learning for Whole Slide Image Classification

#### Jianpeng An, Yunhao Bai, Huazhen Chen, Zhongke Gao, Geert Litjens

[% .details %]
<a class="toggle_visibility" data-selector=".abstract" data-level="3">Show abstract</a>
- <a class="toggle_visibility" data-selector=".schedule" data-level="3">Show schedule</a>
- <a href="">Proceedings</a>
- <a href="https://openreview.net/pdf?id=rV5gzFDn5PF">PDF</a>
- <a href="https://openreview.net/forum?id=rV5gzFDn5PF">Reviews</a>

<p>
    <span class="abstract">
        End-to-end learning with whole-slide digital pathology images is challenging due to their size, which is in the order of gigapixels. In this paper, we propose a novel weakly-supervised learning strategy that combines masked autoencoders (MAE) with multiple instance learning (MIL). We use the output tokens of a self-supervised, pre-trained MAE as instances and design a token selection module to reduce the impact of global average pooling. We evaluate our framework on the assessment of whole-slide image classification on Camelyon16 dataset, showing improved performance compared to the state-of-the-art CLAM algorithm.
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide abstract</a></span>
    </span>
</p>

<p>
    <span class="schedule">
        Wednesday 6th July<br>Poster Session 1.1 - onsite 15:20 - 16:20, virtual 11:00 - 12:00 (UTC+2)
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide schedule</a></span>
    </span>
</p>

[% / %]


---
<!-- { macros.presentation('', '', 720, 450) } -->
